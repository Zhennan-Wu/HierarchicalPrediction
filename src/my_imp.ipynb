{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "98b3a4ca-90d6-445f-a7eb-acb118b97b22",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader, TensorDataset, Dataset\n",
    "import os\n",
    "\n",
    "\n",
    "class Binarize(object):\n",
    "    def __call__(self, tensor):\n",
    "        # Binarize based on a 0.5 threshold (after normalization)\n",
    "        return (tensor > 0.5).float()\n",
    "    \n",
    "\n",
    "class RBM:\n",
    "    def __init__(self, visible_dim, hidden_dim, learning_rate, batch_size, n_iter, verbose, random_state):\n",
    "        # Number of visible and hidden units\n",
    "        self.visible_dim = visible_dim\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.learning_rate = learning_rate\n",
    "        self.batch_size = batch_size\n",
    "        self.n_iter = n_iter\n",
    "        self.verbose = verbose\n",
    "        self.random_state = random_state\n",
    "\n",
    "    def sample_from_prob(self, prob):\n",
    "        # Sampling binary states from probabilities\n",
    "        return torch.bernoulli(prob)\n",
    "\n",
    "    def v_to_h(self, v):\n",
    "        # Propagate visible layer to hidden layer\n",
    "        h_prob = torch.sigmoid(torch.matmul(v, self.W.t()) + self.h_bias)\n",
    "        h_sample = self.sample_from_prob(h_prob)\n",
    "        return h_prob, h_sample\n",
    "\n",
    "    def h_to_v(self, h):\n",
    "        # Propagate hidden layer to visible layer\n",
    "        v_prob = torch.sigmoid(torch.matmul(h, self.W) + self.v_bias)\n",
    "        v_sample = self.sample_from_prob(v_prob)\n",
    "        return v_prob, v_sample\n",
    "    \n",
    "    def persistent_contrastive_divergence(self, v, iter):\n",
    "        # Gibbs Sampling for Persistent Contrastive Divergence\n",
    "        h_pos, _ = self.v_to_h(v)\n",
    "        _, v_neg  = self.h_to_v(self.h_samples_)\n",
    "        h_neg, _ = self.v_to_h(v_neg)\n",
    "        \n",
    "        # Positive and negative phase\n",
    "        pos_phase = torch.matmul(h_pos.t(), v)  # Should match [hidden_dim, visible_dim]\n",
    "        neg_phase = torch.matmul(h_neg.t(), v_neg)  # Should match [hidden_dim, visible_dim]\n",
    "\n",
    "        # Update weights and biases\n",
    "        lr = self.learning_rate/(self.batch_size)\n",
    "        self.W += lr*(pos_phase - neg_phase) / v.size(0)\n",
    "        self.v_bias += lr*torch.sum(v - v_neg, dim=0)\n",
    "        self.h_bias += lr*torch.sum(h_pos - h_neg, dim=0)\n",
    "\n",
    "        # Update the persistent chain\n",
    "        self.h_samples_ = self.sample_from_prob(h_neg)\n",
    "\n",
    "    def _free_energy(self, v):\n",
    "        # Energy function of the RBM\n",
    "        vbias_term = torch.matmul(v, self.v_bias.reshape(-1, 1))\n",
    "        hidden_term = torch.sum(torch.log(1 + torch.exp(torch.matmul(v, self.W.t()) + self.h_bias)), dim=1)\n",
    "        return -vbias_term - hidden_term\n",
    "\n",
    "    def score_samples(self, X):\n",
    "        \"\"\"Compute the pseudo-likelihood of X.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        X : Tensor of shape (n_samples, n_features)\n",
    "            Values of the visible layer. Must be all-boolean (not checked).\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        pseudo_likelihood : Tensor of shape (n_samples,)\n",
    "            Value of the pseudo-likelihood (proxy for likelihood).\n",
    "        \"\"\"\n",
    "        # Ensure X is a PyTorch tensor\n",
    "        X = torch.as_tensor(X).float()\n",
    "\n",
    "        # Randomly corrupt one feature in each sample in X\n",
    "        n_samples, n_features = X.shape\n",
    "        ind = (torch.arange(n_samples), torch.randint(0, n_features, (n_samples,)))\n",
    "\n",
    "        # Create a copy of X and corrupt one feature in each sample\n",
    "        X_corrupted = X.clone()\n",
    "        X_corrupted[ind] = 1 - X_corrupted[ind]\n",
    "\n",
    "        # Calculate free energy for the original and corrupted inputs\n",
    "        fe = self._free_energy(X)\n",
    "        fe_corrupted = self._free_energy(X_corrupted)\n",
    "\n",
    "        # Compute the pseudo-likelihood using the logistic function of the difference\n",
    "        pseudo_likelihood = -n_features * torch.logaddexp(torch.tensor(0.0), -(fe_corrupted - fe))\n",
    "\n",
    "        return pseudo_likelihood\n",
    "\n",
    "    def encoder(self, dataset: torch.Tensor, label: torch.Tensor) -> torch.Tensor:\n",
    "        \"\"\"\n",
    "        Generate top level latent variables\n",
    "        \"\"\"\n",
    "        p_h_given_v, _ = self.v_to_h(dataset)\n",
    "        return p_h_given_v\n",
    "\n",
    "    def encode(self, dataloader: DataLoader) -> DataLoader:\n",
    "        \"\"\"\n",
    "        Encode data\n",
    "        \"\"\"\n",
    "        latent_vars = []\n",
    "        labels = []\n",
    "        for data, label in dataloader:\n",
    "            data = data.view(-1, self.visible_dim)\n",
    "            label = label.unsqueeze(1).to(torch.float32)\n",
    "            latent_vars.append(self.encoder(data, label))\n",
    "            labels.append(label)\n",
    "        latent_vars = torch.cat(latent_vars, dim=0)\n",
    "        labels = torch.cat(labels, dim=0)\n",
    "        latent_dataset = TensorDataset(latent_vars, labels)\n",
    "\n",
    "        return DataLoader(latent_dataset, batch_size=dataloader.batch_size, shuffle=False)\n",
    "    \n",
    "    def fit(self, data_loader: DataLoader, k=1):\n",
    "        \"\"\"\n",
    "        Fit the RBM model\n",
    "        \"\"\"\n",
    "        self.W = torch.randn(self.hidden_dim, self.visible_dim, dtype=torch.float32)\n",
    "        self.v_bias = torch.zeros(self.visible_dim, dtype=torch.float32)\n",
    "        self.h_bias = torch.zeros(self.hidden_dim, dtype=torch.float32)\n",
    "        self.h_samples_ = torch.zeros(self.batch_size, self.hidden_dim, dtype=torch.float32)\n",
    "        \n",
    "        for epoch in range(self.n_iter):\n",
    "            epoch_loss = 0\n",
    "            for batch in data_loader:\n",
    "                v, _ = batch  # Ignore labels\n",
    "                v = v.view(-1, self.visible_dim)  # Flatten the input images to [batch_size, visible_dim]\n",
    "                # Update parameters\n",
    "                self.persistent_contrastive_divergence(v, epoch)\n",
    "\n",
    "            print(f\"Epoch {epoch+1}/{self.n_iter} finished\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "3239d97f-0695-4ac6-abc6-c3ff84861c67",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([64, 28, 28])\n",
      "torch.Size([64, 784])\n"
     ]
    }
   ],
   "source": [
    "from torchvision import datasets, transforms\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "# Load the MNIST dataset\n",
    "transform = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    Binarize()\n",
    "])\n",
    "mnist = datasets.MNIST(root='./data', train=True, download=True, transform=transform)\n",
    "data_loader = DataLoader(mnist, batch_size=64, shuffle=False, drop_last=True)\n",
    "\n",
    "for (data, label) in data_loader:\n",
    "    print(data.squeeze(dim=1).shape)\n",
    "    print(data.view(-1, 28*28).shape)\n",
    "    break\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "2fdc6738-7d62-4cda-a0df-c77a06f6a4fc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10 finished\n",
      "Epoch 2/10 finished\n",
      "Epoch 3/10 finished\n",
      "Epoch 4/10 finished\n",
      "Epoch 5/10 finished\n",
      "Epoch 6/10 finished\n",
      "Epoch 7/10 finished\n",
      "Epoch 8/10 finished\n",
      "Epoch 9/10 finished\n",
      "Epoch 10/10 finished\n"
     ]
    }
   ],
   "source": [
    "# Initialize and train the RBM\n",
    "visible_dim = 28 * 28  # MNIST images are 28x28\n",
    "hidden_dim = 128       # You can adjust this value\n",
    "learning_rate = 0.06\n",
    "batch_size = 64\n",
    "n_iter = 10\n",
    "verbose = 1\n",
    "random_state = 42\n",
    "rbm = RBM(visible_dim, hidden_dim, learning_rate, batch_size, n_iter, verbose, random_state)\n",
    "\n",
    "rbm.fit(data_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "a2b2a6b1-2bf3-4ca9-969c-11898c19b59f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-1.1653,  0.8080, -0.1664,  ..., -0.5660,  0.3053,  0.4966],\n",
       "        [ 0.3320, -0.3331, -0.9235,  ..., -0.1498, -1.1905,  0.6766],\n",
       "        [ 0.2480,  1.1154, -0.6515,  ..., -1.5787, -0.8422,  0.6336],\n",
       "        ...,\n",
       "        [-0.6814, -0.2682,  0.9927,  ...,  0.5435,  0.9702,  1.1919],\n",
       "        [ 2.2777, -1.0583, -0.3714,  ...,  0.4620, -0.1399,  0.6953],\n",
       "        [-0.9509,  1.8501,  1.3058,  ..., -1.3820,  1.3236,  0.2145]])"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rbm.W"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "db78b721-d3d9-4d87-8488-1f236553b21a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([128, 784])"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rbm.W.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "67cb15ef-b018-45e6-97c6-c0733e241fbe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Features shape: (70000, 784)\n",
      "Labels shape: (70000,)\n",
      "First 5 labels: [5 0 4 1 9]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from sklearn.datasets import fetch_openml\n",
    "\n",
    "# Load the MNIST dataset from OpenML\n",
    "mnist = fetch_openml('mnist_784', version=1, as_frame=False)\n",
    "\n",
    "# Extract features and labels\n",
    "X, y = mnist['data'], mnist['target']\n",
    "\n",
    "# Convert labels to integers\n",
    "y = y.astype(np.uint8)\n",
    "\n",
    "# Check the data shapes and some samples\n",
    "print('Features shape:', X.shape)  # Should be (70000, 784)\n",
    "print('Labels shape:', y.shape)     # Should be (70000,)\n",
    "print('First 5 labels:', y[:5])     # Check the first 5 labels\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "a5688c24-7841-417d-8e10-e87cfd0c1191",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training set shape: (56000, 784) (56000,)\n",
      "Testing set shape: (14000, 784) (14000,)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "print(\"Training set shape:\", X_train.shape, y_train.shape)\n",
    "print(\"Testing set shape:\", X_test.shape, y_test.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "782717a0-e46a-47de-8d5a-b623cfdffabe",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "hdp-kernel",
   "language": "python",
   "name": "hdp-kernel"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
