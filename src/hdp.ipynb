{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "37078a52-aa83-453d-994e-a9c0e1d550cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import pyro\n",
    "import itertools\n",
    "import jax\n",
    "import jax.numpy as jnp\n",
    "\n",
    "from pyro.distributions import Dirichlet, Gamma, Categorical\n",
    "from torch.multiprocessing import Pool\n",
    "from typing import Union"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "798949c7-e8db-471a-b28b-df7c2492ebf3",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Categorical_Distribution:\n",
    "    def __init__(self, weights: torch.Tensor, values: torch.Tensor):\n",
    "        '''\n",
    "        Initialize a Categorical Distribution with weights\n",
    "\n",
    "        Parameters:\n",
    "        - weights (list): the weights of the values\n",
    "        - values (list): the values of the Categorical Distribution\n",
    "        '''\n",
    "        self.weights = weights\n",
    "        self.values = values\n",
    "    \n",
    "    def sample(self):\n",
    "        '''\n",
    "        Sample from the Categorical Distribution \n",
    "        '''\n",
    "        idx = Categorical(self.weights).sample()\n",
    "        return self.values[idx.item()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ec8671be-d83b-440a-998f-25822130903b",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DirichletProcess:\n",
    "    def __init__(self, alpha: float, sample_size: int, base_distribution: dict):\n",
    "        '''\n",
    "        Initialize a Dirichlet Process with concentration parameter alpha\n",
    "\n",
    "        Parameters:\n",
    "        - alpha (float): the concentration parameter of the Dirichlet Process\n",
    "        - base_distribution (Dict): the base distribution of the Dirichlet Process\n",
    "        '''\n",
    "        self.alpha = alpha\n",
    "        self.values = []\n",
    "        self.weights = []\n",
    "        self.base_distribution = Categorical_Distribution(base_distribution[\"weights\"], base_distribution[\"values\"])\n",
    "        self.sample(sample_size)\n",
    "    \n",
    "    def sample(self, num_samples: int):\n",
    "        '''\n",
    "        Sample from the Dirichlet Process\n",
    "\n",
    "        Parameters:\n",
    "        - num_samples (int): the number of samples to draw\n",
    "        '''\n",
    "        # Sample from the base distribution with probability alpha / (alpha + N)\n",
    "        for _ in range(num_samples):\n",
    "            total_counts = sum(self.weights)\n",
    "            probs = torch.tensor(self.weights) / total_counts\n",
    "            p_existing = self.alpha / (self.alpha + total_counts)\n",
    "            if (torch.rand(1) > p_existing):\n",
    "                # Select existing sample\n",
    "                idx = Categorical(probs).sample()\n",
    "                self.weights[idx] += 1\n",
    "            else:\n",
    "                # Sample from the base distribution\n",
    "                new_entry = self.base_distribution.sample()\n",
    "                unseen = True\n",
    "                for index, entry in enumerate(self.values):\n",
    "                    if (torch.equal(entry, new_entry)):\n",
    "                        self.weights[index] += 1\n",
    "                        unseen = False\n",
    "                        break\n",
    "                if (unseen):\n",
    "                    self.values.append(new_entry)\n",
    "                    self.weights.append(1)\n",
    "    \n",
    "    def get_values(self):\n",
    "        '''\n",
    "        Get the entries in the Dirichlet Process\n",
    "        '''\n",
    "        return self.values\n",
    "    \n",
    "    def get_weights(self):\n",
    "        '''\n",
    "        Get the weights of the entries in the Dirichlet Process\n",
    "        '''\n",
    "        return self.weights\n",
    "\n",
    "    def get_distribution(self):\n",
    "        '''\n",
    "        Get the distribution of the Dirichlet Process\n",
    "        '''\n",
    "        return {\"values\": torch.stack(self.values), \"weights\": torch.tensor(self.weights)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "cff7ff92-8cb9-493a-a2bf-433bcae7e4d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "class HierarchicalDirichletProcess:\n",
    "    def __init__(self, num_of_words: int, layers: int, sample_size: int, fixed_layers: dict = None, global_sample_size: int = 1000):\n",
    "        '''\n",
    "        Initialize a Hierarchical Dirichlet Process with layers\n",
    "\n",
    "        Parameters:\n",
    "        - num_of_words (int): the number of words in the vocabulary\n",
    "        - layers (int): the number of layers in the Hierarchical Dirichlet Process\n",
    "        - fixed_layers (dict): the fixed number of categories in each layer\n",
    "        - global_sample_size (int): the number of samples to draw from the Global Dirichlet Process\n",
    "        '''\n",
    "        self.layers = layers\n",
    "        self.layer_constrains = False\n",
    "        self.implied_constraints = None\n",
    "        self.fixed_layers = fixed_layers\n",
    "        self._check_layer_constraints()\n",
    "\n",
    "        self.category_hierarchy = []\n",
    "        # Initialize Global Dirichlet Process\n",
    "        gamma = 100\n",
    "        self.global_dist, self.true_params = self.generate_Global_DP(num_of_words, global_sample_size, gamma)\n",
    "        print(\"Global Base Distribution\")\n",
    "        print(self.global_dist)\n",
    "        print(\"True Parameters\")\n",
    "        print(self.true_params)\n",
    "\n",
    "        # Initialize HDP variables (not finished yet)\n",
    "        eta = Gamma(1, 1).sample()\n",
    "        self.labels = self.generate_nCRP(sample_size, eta)\n",
    "        self.num_categories_per_layer, self.hierarchy_tree = self.summarize_nCRP(self.labels)\n",
    "        self.hdp = self.generate_HDP(sample_size, self.hierarchy_tree, self.labels)\n",
    "    \n",
    "    def _check_layer_constraints(self):\n",
    "        '''\n",
    "        Check if the layer constraints are satisfied\n",
    "        '''\n",
    "        if (self.fixed_layers is not None):\n",
    "            fix_keys = list(self.fixed_layers.keys())\n",
    "            fix_values = list(self.fixed_layers.values())\n",
    "            if (len(fix_keys) > 1):\n",
    "                if (all(fix_keys[i] <= fix_keys[i+1] for i in range(len(fix_keys)-1))):\n",
    "                    raise ValueError(\"The fixed layers should be in increasing order, get {}\".format(fix_keys))\n",
    "                if (all(fix_values[i] <= fix_values[i+1] for i in range(len(fix_values)-1))):\n",
    "                    raise ValueError(\"The fixed layers should have increasing number of categories, get{}\".format(fix_values))\n",
    "            self.layer_constrains = True\n",
    "            layer_index = [float('inf')]*self.layers\n",
    "            self.implied_constraints = {}\n",
    "            for i in range(self.layers):\n",
    "                if (i in self.fixed_layers.keys()):\n",
    "                    layer_index[i] = self.fixed_layers[i]\n",
    "            for i in range(self.layers):\n",
    "                self.implied_constraints[i] = min(layer_index[i:])\n",
    "        \n",
    "    def generate_Global_DP(self, dimension: int, num_samples: int, gamma: float):\n",
    "        '''\n",
    "        Generate a Global Dirichlet Process with num_categories and concentration parameter gamma\n",
    "\n",
    "        Parameters:\n",
    "        - num_categories (int): the number of categories to generate\n",
    "        - gamma (float): the concentration parameter of the Global Dirichlet Process\n",
    "\n",
    "        Returns:\n",
    "        - values (torch.Tensor): the values of the Global Dirichlet Process\n",
    "        - weights (torch.Tensor): the weights of the values in the Global Dirichlet Process\n",
    "        '''\n",
    "        weights = []\n",
    "        values = []\n",
    "        base_param = Gamma(1, 1).sample()\n",
    "        base_distribution = Dirichlet(base_param * torch.ones(dimension))\n",
    "        for _ in range(num_samples):\n",
    "            total_counts = sum(weights)\n",
    "            probs = torch.tensor(weights) / total_counts\n",
    "            p_existing = gamma / (gamma + total_counts)\n",
    "            if (torch.rand(1) > p_existing):\n",
    "                # Select existing sample\n",
    "                idx = Categorical(probs).sample()\n",
    "                weights[idx] += 1\n",
    "            else:\n",
    "                # Sample from the base distribution\n",
    "                new_entry = base_distribution.sample()\n",
    "                unseen = True\n",
    "                for index, entry in enumerate(values):\n",
    "                    if (torch.equal(entry, new_entry)):\n",
    "                        weights[index] += 1\n",
    "                        unseen = False\n",
    "                        break\n",
    "                if (unseen):\n",
    "                    values.append(new_entry)\n",
    "                    weights.append(1)\n",
    "        # print(\"Global Dirichlet Process\")\n",
    "        # print(\"Values: \", values)\n",
    "        # print(\"Weights: \", weights)\n",
    "        base_dist = {\"values\": torch.arange(len(weights)), \"weights\": torch.tensor(weights)}\n",
    "        # print(\"Base Distribution\")\n",
    "        # print(base_dist)\n",
    "        ground_truth = torch.stack(values)\n",
    "        return base_dist, ground_truth\n",
    "\n",
    "    def summarize_CRP(self, labels: Union[torch.Tensor, list], indices: Union[torch.Tensor, list]):\n",
    "        '''\n",
    "        Summarize the Chinese Restaurant Process to get the unique values and their counts\n",
    "\n",
    "        Parameters:\n",
    "        - labels (torch.Tensor or list): the labels of the samples\n",
    "\n",
    "        Returns:\n",
    "        - unique_values (torch.Tensor): the unique values in the labels tensor or list of tensors (same value in different tensors are considered as different values)\n",
    "        - counts (torch.Tensor): the counts of the unique values in the labels tensor or list of tensors (same value in different tensors are considered as different values)\n",
    "        '''\n",
    "        unique_values_list = []\n",
    "        label_indices_list = []\n",
    "        counts_list = []\n",
    "        if (isinstance(labels, list)):\n",
    "            p_cat = 0\n",
    "            category_dict = {}\n",
    "            for label, index in zip(labels, indices):\n",
    "                unique_values, inverse_indices, counts = torch.unique(label, return_inverse = True, return_counts=True)\n",
    "                category_dict[p_cat] = {val.item(): {} for val in unique_values}\n",
    "                p_cat += 1\n",
    "                for i in range(unique_values.shape[0]):\n",
    "                    label_indices_list.append(index[torch.where(inverse_indices == i)])\n",
    "                unique_values_list.append(unique_values)\n",
    "                counts_list.append(counts)\n",
    "            unique_values = torch.cat(unique_values_list, dim=0)\n",
    "            counts = torch.cat(counts_list, dim=0)\n",
    "            self.category_hierarchy.append(category_dict)\n",
    "        else:\n",
    "            unique_values, inverse_indices, counts = torch.unique(labels, return_inverse = True, return_counts=True)\n",
    "            self.category_hierarchy.append({val.item(): {} for val in unique_values})\n",
    "            for i in range(unique_values.shape[0]):\n",
    "                label_indices_list.append(indices[torch.where(inverse_indices == i)])\n",
    "        return unique_values, label_indices_list, counts\n",
    "    \n",
    "    def generate_CRP(self, sample_size: int, eta: float):\n",
    "        '''\n",
    "        Generate a Chinese Restaurant Process with sample size sample_size and concentration parameter eta\n",
    "        \n",
    "        Parameters:\n",
    "        - sample_size (int): the number of samples to generate labels for\n",
    "        - eta (float): the concentration parameter of the Chinese Restaurant Process\n",
    "\n",
    "        Returns:\n",
    "        - labels (torch.Tensor): the labels of the samples\n",
    "        '''\n",
    "        labels = torch.tensor([0], dtype=torch.int32)\n",
    "        for _ in range(1, sample_size):\n",
    "            unique_values, counts = torch.unique(labels, return_counts=True)\n",
    "            if (torch.rand(1) < eta/(eta + torch.sum(counts))):\n",
    "                # Add a new label\n",
    "                new_label = torch.max(unique_values) + 1\n",
    "                labels = torch.cat((labels, new_label.unsqueeze(0)), dim=0)\n",
    "            else:\n",
    "                # Select an existing label\n",
    "                new_label = Categorical(counts).sample()\n",
    "                labels = torch.cat((labels, new_label.unsqueeze(0)), dim=0)\n",
    "        return labels\n",
    "\n",
    "    def generate_fixed_categories(self, parent_categories_counts: torch.Tensor, eta: float, num_categories: int):\n",
    "        '''\n",
    "        Generate a Chinese Restaurant Process with sample size sample_size and concentration parameter eta with fixed number of categories\n",
    "\n",
    "        Parameters:\n",
    "        - parent_categories_counts (torch.Tensor): the number of samples in each parent category\n",
    "        - eta (float): the concentration parameter of the Chinese Restaurant Process\n",
    "        - num_categories (int): the number of child categories to generate\n",
    "\n",
    "        Returns:\n",
    "        - labels (torch.Tensor): the labels of the samples\n",
    "        '''\n",
    "        num_parent_categories = parent_categories_counts.shape[0]\n",
    "        if (num_categories < num_parent_categories):\n",
    "            raise ValueError(\"The number of child categories should be greater than the number of parent categories\")\n",
    "        # Randomly assign child categories to parent categories\n",
    "        child_categories = list(range(num_categories))\n",
    "        child_categories = [[x] for x in child_categories]\n",
    "        parent_child_relation = dict(zip(list(range(num_categories)),child_categories))\n",
    "        additional_categories = num_categories - num_parent_categories\n",
    "        for i in range(additional_categories):\n",
    "            parent_category = torch.randint(0, num_parent_categories, (1,))\n",
    "            parent_child_relation[parent_category.item()].append(i+num_parent_categories)\n",
    "        parent_categories_counts = parent_categories_counts.to(torch.int).tolist()\n",
    "        parent_child_list = list(parent_child_relation.values())\n",
    "\n",
    "        # Generate instances based on the assigned parent-child categories\n",
    "        labels = []\n",
    "        for p_count, p_c in zip(parent_categories_counts, parent_child_list):\n",
    "            # Generate the first instance under the parent category\n",
    "            p_labels = []\n",
    "            p_labels_record = []\n",
    "            candidates = p_c\n",
    "            new_label = candidates[torch.randint(0, len(candidates), (1,)).item()]\n",
    "            p_labels_record.append(new_label) \n",
    "            p_labels.append(0)    \n",
    "            # Chinese resutaurant process to generate the rest of the instances      \n",
    "            for _ in range(p_count-1):\n",
    "                counts = len(set(p_labels_record))\n",
    "                candidates = list(set(p_c) - set(p_labels_record))\n",
    "                if (torch.rand(1) < eta/(eta + counts) and len(candidates) > 0):\n",
    "                    # Add a new label\n",
    "                    new_label = candidates[torch.randint(0, len(candidates), (1,)).item()]\n",
    "                    p_labels_record.append(new_label)\n",
    "                    p_labels.append(max(p_labels)+1)\n",
    "                else:\n",
    "                    unique_values, counts = torch.unique(torch.tensor(p_labels), return_counts=True)\n",
    "                    new_label_index = Categorical(counts).sample().item()\n",
    "                    new_label = unique_values[new_label_index].item()\n",
    "                    p_labels.append(new_label)\n",
    "            labels.append(torch.tensor(p_labels))\n",
    "        return labels\n",
    "    \n",
    "    def generate_nCRP(self, sample_size: int, eta: float):\n",
    "        '''\n",
    "        Generate a nested Chinese Restaurant Process with sample size sample_size and concentration parameter eta\n",
    "        \n",
    "        Parameters:\n",
    "        - sample_size (int): the number of samples to generate labels for\n",
    "        - eta (float): the concentration parameter of the Chinese Restaurant Process\n",
    "\n",
    "        Returns:\n",
    "        - label_hierarchy (torch.Tensor): the labels of the samples in the nested Chinese Restaurant Process\n",
    "        '''\n",
    "        label_hierarchy = []\n",
    "        prev_labels = self.generate_CRP(sample_size, eta)\n",
    "        prev_indices = torch.arange(sample_size)\n",
    "        prev_unique_values = torch.zeros(1)\n",
    "        prev_counts = torch.tensor([sample_size])\n",
    "        label_hierarchy.append(prev_labels)\n",
    "        l = 1\n",
    "        while (l < self.layers):\n",
    "            unique_values, indices, counts = self.summarize_CRP(prev_labels, prev_indices)\n",
    "            num_categories = unique_values.shape[0]\n",
    "            if (num_categories > self.implied_constraints[l]):\n",
    "                label_hierarchy.pop()\n",
    "                if (l == 1):\n",
    "                    indices = torch.arange(sample_size).unsqueeze(0)\n",
    "                    unique_values = torch.zeros(1).unsqueeze(0)\n",
    "                    counts = torch.tensor(sample_size).unsqueeze(0)\n",
    "                    num_categories = 1\n",
    "                else:\n",
    "                    indices = prev_indices\n",
    "                    unique_values = prev_unique_values\n",
    "                    counts = prev_counts\n",
    "                    num_categories = unique_values.shape[0]\n",
    "                num_subcategories = self.implied_constraints[l]\n",
    "                labels =self.generate_fixed_categories(counts, eta, num_subcategories)   \n",
    "                l -= 1\n",
    "            elif (l in list(self.fixed_layers.keys())):\n",
    "                num_subcategories = self.fixed_layers[l]\n",
    "                labels =self.generate_fixed_categories(counts, eta, num_subcategories)               \n",
    "            else:\n",
    "                with Pool(num_categories) as p:\n",
    "                    params =list(itertools.product(counts.tolist(), [eta]))\n",
    "                    labels = p.starmap(self.generate_CRP, params)\n",
    "            if (isinstance(indices, list)):\n",
    "                global_indices = torch.cat(indices, dim=0)\n",
    "            else:\n",
    "                global_indices = indices\n",
    "            new_layer_label = torch.zeros(sample_size, dtype=torch.long)\n",
    "            new_layer_label[global_indices] = torch.cat(labels, dim=0)\n",
    "            label_hierarchy.append(new_layer_label)\n",
    "\n",
    "            prev_labels = labels\n",
    "            prev_indices = indices\n",
    "            prev_unique_values = unique_values\n",
    "            prev_counts = counts\n",
    "            l += 1\n",
    "\n",
    "        return torch.stack(label_hierarchy, dim=0).t()\n",
    "\n",
    "    def summarize_nCRP(self, label_hierarchy: torch.Tensor):\n",
    "        '''\n",
    "        Summarize the nested Chinese Restaurant Process to get the unique values and their counts\n",
    "\n",
    "        Parameters:\n",
    "        - label_hierarchy (torch.Tensor): the labels of the samples in the nested Chinese Restaurant Process\n",
    "\n",
    "        Returns:\n",
    "        - unique_values (torch.Tensor): the unique values in the labels tensor or list of tensors (same value in different tensors are considered as different values)\n",
    "        - counts (torch.Tensor): the counts of the unique values in the labels tensor or list of tensors (same value in different tensors are considered as different values)\n",
    "        '''\n",
    "\n",
    "        num_categories_per_layer = {}\n",
    "        for l in range(self.layers):\n",
    "            unique_values = torch.unique(label_hierarchy[:, :l+1], dim=0)\n",
    "            num_categories_per_layer[l] = unique_values.shape[0]\n",
    "\n",
    "        hierarchy_tree = {}\n",
    "        for entry in label_hierarchy:\n",
    "            level = hierarchy_tree\n",
    "            for l, category in enumerate(entry):\n",
    "                if category.item() not in level:\n",
    "                    if (l == self.layers - 1):\n",
    "                        level[category.item()] = 1\n",
    "                    else:\n",
    "                        level[category.item()] = {}\n",
    "                else:\n",
    "                    if (l == self.layers - 1):\n",
    "                        level[category.item()] += 1\n",
    "                level = level[category.item()]\n",
    "\n",
    "        return num_categories_per_layer, hierarchy_tree\n",
    "    \n",
    "    def _extract_child_layer(self, parent_layer: list):\n",
    "        '''\n",
    "        Extract the child layers from the parent layers\n",
    "\n",
    "        Parameters:\n",
    "        - parent_layer (list): the parent layer of list of hierarchical dictionaries\n",
    "\n",
    "        Returns:    \n",
    "        - child_layer (list): the child layer of list of hierarchical dictionaries\n",
    "        '''\n",
    "        child_layer = []\n",
    "        counts = []\n",
    "        child_sample_sizes = []\n",
    "        for parent in parent_layer:\n",
    "            child = list(parent.values())\n",
    "            sample_size = []\n",
    "            for c in child:\n",
    "                leaves = jax.tree_util.tree_leaves(c)\n",
    "                sample_size.append(sum(leaves))\n",
    "            child_sample_sizes = child_sample_sizes + sample_size\n",
    "            count = len(child)\n",
    "            child_layer = child_layer + child\n",
    "            counts.append(count)\n",
    "        return child_layer, child_sample_sizes, counts\n",
    "\n",
    "    def update_hierarchy_dict(self, Distributions: list, counts: list, labels: torch.Tensor, parent_hierarchy: dict = None):\n",
    "        '''\n",
    "        Update the hierarchy tree with the distributions\n",
    "        '''\n",
    "        child_dict = {}\n",
    "        distribution_params = []\n",
    "        for d in Distributions:\n",
    "            distribution_params.append(d.get_distribution())\n",
    "        if (parent_hierarchy is None):\n",
    "            child_keys = list(range(len(Distributions)))\n",
    "            child_keys = [str(key) for key in child_keys]\n",
    "            child_dict = dict(zip(child_keys, distribution_params))\n",
    "        else:\n",
    "            parent_keys = list(parent_hierarchy.keys())\n",
    "            if (len(parent_keys) != len(counts)):\n",
    "                raise ValueError(\"The number of parent keys {} should be equal to the number of counts {}\".format(len(parent_keys), len(counts)))\n",
    "            child_keys = []\n",
    "            for key, count in zip(parent_keys, counts):\n",
    "                child_key =  []\n",
    "                for c in range(count):\n",
    "                    child_key.append(str(key) + str(c))\n",
    "                child_keys = child_keys + child_key\n",
    "            if (len(child_keys) != len(Distributions)):\n",
    "                raise ValueError(\"The number of child keys {} should be equal to the number of Distributions {}\".format(len(child_keys), len(Distributions)))\n",
    "            child_dict = dict(zip(child_keys, distribution_params))\n",
    "        return child_dict\n",
    "\n",
    "    def generate_HDP(self, sample_size: int, hierarchy_tree: dict, labels: torch.Tensor):\n",
    "        '''\n",
    "        Generate a Hierarchical Dirichlet Process\n",
    "        '''\n",
    "        HDP_distributions_no_duplicate = []\n",
    "        gamma = Gamma(1, 1).sample()\n",
    "        Global = DirichletProcess(gamma, 10*sample_size, self.global_dist) \n",
    "        HDP_structure = []\n",
    "        HDP_distributions = []\n",
    "        HDP_sample_sizes = []\n",
    "        counts = [len(list(hierarchy_tree.keys()))]\n",
    "        HDP_distributions.append([Global.get_distribution()]*counts[0])\n",
    "        HDP_distributions_no_duplicate.append([Global.get_distribution()])\n",
    "        sample_sizes = []\n",
    "        for c in list(hierarchy_tree.values()):\n",
    "            leaves = jax.tree_util.tree_leaves(c)\n",
    "            sample_sizes.append(sum(leaves))\n",
    "        HDP_sample_sizes.append(sample_sizes)\n",
    "        level = list(hierarchy_tree.values())\n",
    "        for l in range(self.layers):\n",
    "            alpha = Gamma(1, 1).sample()\n",
    "            base = HDP_distributions[-1]\n",
    "            base_sample_sizes = HDP_sample_sizes[-1]\n",
    "            alpha_list = [alpha.item()]*len(base_sample_sizes)\n",
    "            param = list(zip(alpha_list, base_sample_sizes, base))\n",
    "            with Pool(len(base)) as p:\n",
    "                DPs = p.starmap(DirichletProcess, param)\n",
    "            if (l == 0):\n",
    "                HDP_structure.append(self.update_hierarchy_dict(DPs, counts, labels))\n",
    "            else:\n",
    "                HDP_structure.append(self.update_hierarchy_dict(DPs, counts, labels, HDP_structure[-1]))\n",
    "            if (l < self.layers - 1):\n",
    "                level, sample_sizes, counts = self._extract_child_layer(level)\n",
    "                child_distributions = []\n",
    "                child_distributions_no_duplicate = []\n",
    "                if (len(counts) != len(DPs)):\n",
    "                    raise ValueError(\"The number of child layers {} should be equal to the number of Dirichlet Processes {}\".format(len(counts), len(DPs)))\n",
    "                for count, DP in zip(counts, DPs):\n",
    "                    child_distributions_no_duplicate.append(DP.get_distribution())\n",
    "                    next_base = [DP.get_distribution()]*count\n",
    "                    child_distributions = child_distributions + next_base\n",
    "                HDP_distributions.append(child_distributions)\n",
    "                HDP_sample_sizes.append(sample_sizes)\n",
    "                HDP_distributions_no_duplicate.append(child_distributions_no_duplicate)\n",
    "            else:\n",
    "                sample_sizes = level\n",
    "                counts = [1]*len(level)\n",
    "                child_distributions = []\n",
    "                child_distributions_no_duplicate = []\n",
    "                if (len(counts) != len(DPs)):\n",
    "                    raise ValueError(\"The number of child layers {} should be equal to the number of Dirichlet Processes {}\".format(len(counts), len(DPs)))\n",
    "                for count, DP in zip(counts, DPs):\n",
    "                    child_distributions_no_duplicate.append(DP.get_distribution())\n",
    "                    next_base = [DP.get_distribution()]*count\n",
    "                    child_distributions = child_distributions + next_base\n",
    "                HDP_distributions_no_duplicate.append(child_distributions_no_duplicate)\n",
    "                HDP_distributions.append(child_distributions)\n",
    "                HDP_sample_sizes.append(sample_sizes)\n",
    "        self._check_nCRP_HDP_match(labels, HDP_structure)\n",
    "        return HDP_distributions_no_duplicate, HDP_structure\n",
    "\n",
    "    def _check_nCRP_HDP_match(self, nCRP_hierarchy: torch.Tensor, HDP_hierarchy: list):\n",
    "        '''\n",
    "        Check if the nCRP and HDP hierarchies match\n",
    "        '''\n",
    "        for idx, level_HDP in enumerate(HDP_hierarchy):\n",
    "            level_nCRP = nCRP_hierarchy[:, :idx+1]\n",
    "            level_nCRP_list = level_nCRP.tolist()\n",
    "            level_nCRP_keys = [''.join(map(str, row)) for row in level_nCRP_list]\n",
    "            for key in level_nCRP_keys:\n",
    "                if key not in level_HDP.keys():\n",
    "                    raise ValueError(\"The nCRP hierarchy {} does not match the HDP hierarchy {}\".format(key, list(level_HDP.keys())))\n",
    "        print(\"The nCRP hierarchy matches the HDP hierarchy\")\n",
    "        \n",
    "    def visualize_HDP(self, HDP_distributions: list, labels: torch.Tensor):\n",
    "        '''\n",
    "        Visualize the Hierarchical Dirichlet Process\n",
    "        '''\n",
    "        for l, distributions in enumerate(HDP_distributions):\n",
    "            print(\"Layer: \", l)\n",
    "            print(\"Number of subclass: \", len(distributions))\n",
    "            for d in distributions:\n",
    "                print(\"Values: \", d[\"values\"], \"Weights: \", d[\"weights\"])\n",
    "    \n",
    "    def calculate_likelihood(self, variable_indices, HDP_distributions: list, labels: torch.Tensor):\n",
    "        '''\n",
    "        Calculate the likelihood of the Hierarchical Dirichlet Process\n",
    "        '''\n",
    "        pass\n",
    "\n",
    "    def infer_HDP(self, HDP_distributions: list, labels: torch.Tensor, hdp_structure: list):\n",
    "        '''\n",
    "        Infer the Hierarchical Dirichlet Process layer by layer\n",
    "        '''\n",
    "        \n",
    "        pass\n",
    "    \n",
    "    def match_HDP(self, HDP_distributions: list, hierarchy_tree: dict):\n",
    "        '''\n",
    "        Match the Hierarchical Dirichlet Process\n",
    "        '''\n",
    "        pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "edb3aaa5-d0ca-456a-96f0-2aff199c1817",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch import tensor\n",
    "labels = tensor([[0, 0, 0],\n",
    "        [0, 0, 0],\n",
    "        [1, 0, 0],\n",
    "        [0, 0, 0],\n",
    "        [0, 0, 1],\n",
    "        [0, 0, 1],\n",
    "        [2, 0, 0],\n",
    "        [1, 0, 0],\n",
    "        [0, 0, 2],\n",
    "        [0, 0, 3],\n",
    "        [0, 0, 4],\n",
    "        [3, 0, 0],\n",
    "        [0, 0, 1],\n",
    "        [0, 1, 0],\n",
    "        [0, 0, 5],\n",
    "        [1, 0, 0],\n",
    "        [0, 0, 1],\n",
    "        [0, 1, 0],\n",
    "        [0, 0, 1],\n",
    "        [3, 1, 0],\n",
    "        [1, 0, 0],\n",
    "        [0, 0, 1],\n",
    "        [3, 0, 1],\n",
    "        [1, 0, 0],\n",
    "        [0, 0, 0],\n",
    "        [0, 0, 3],\n",
    "        [0, 0, 3],\n",
    "        [0, 0, 1],\n",
    "        [0, 0, 4],\n",
    "        [0, 0, 4],\n",
    "        [1, 0, 0],\n",
    "        [0, 0, 4],\n",
    "        [1, 1, 0],\n",
    "        [0, 1, 0],\n",
    "        [1, 0, 0],\n",
    "        [0, 0, 5],\n",
    "        [0, 0, 1],\n",
    "        [4, 0, 0],\n",
    "        [0, 0, 1],\n",
    "        [3, 2, 0],\n",
    "        [0, 0, 1],\n",
    "        [3, 0, 1],\n",
    "        [0, 1, 1],\n",
    "        [0, 0, 5],\n",
    "        [2, 1, 0],\n",
    "        [0, 1, 2],\n",
    "        [0, 0, 6],\n",
    "        [0, 0, 1],\n",
    "        [2, 1, 0],\n",
    "        [0, 1, 3]])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "d759549b-092f-467f-bb89-7a3bd02dab73",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch import tensor\n",
    "HDP = [[{'values': tensor([22,  3]), 'weights': tensor([643, 357])}], [{'values': tensor([22,  3]), 'weights': tensor([32,  1])}, {'values': tensor([22]), 'weights': tensor([8])}, {'values': tensor([22]), 'weights': tensor([3])}, {'values': tensor([22]), 'weights': tensor([5])}, {'values': tensor([3]), 'weights': tensor([1])}], [{'values': tensor([22]), 'weights': tensor([27])}, {'values': tensor([22]), 'weights': tensor([6])}, {'values': tensor([22]), 'weights': tensor([7])}, {'values': tensor([22]), 'weights': tensor([1])}, {'values': tensor([22]), 'weights': tensor([1])}, {'values': tensor([22]), 'weights': tensor([2])}, {'values': tensor([22]), 'weights': tensor([3])}, {'values': tensor([22]), 'weights': tensor([1])}, {'values': tensor([22]), 'weights': tensor([1])}, {'values': tensor([3]), 'weights': tensor([1])}], [{'values': tensor([22]), 'weights': tensor([4])}, {'values': tensor([22]), 'weights': tensor([11])}, {'values': tensor([22]), 'weights': tensor([1])}, {'values': tensor([22]), 'weights': tensor([3])}, {'values': tensor([22]), 'weights': tensor([4])}, {'values': tensor([22]), 'weights': tensor([3])}, {'values': tensor([22]), 'weights': tensor([1])}, {'values': tensor([22]), 'weights': tensor([3])}, {'values': tensor([22]), 'weights': tensor([1])}, {'values': tensor([22]), 'weights': tensor([1])}, {'values': tensor([22]), 'weights': tensor([1])}, {'values': tensor([22]), 'weights': tensor([7])}, {'values': tensor([22]), 'weights': tensor([1])}, {'values': tensor([22]), 'weights': tensor([1])}, {'values': tensor([22]), 'weights': tensor([2])}, {'values': tensor([22]), 'weights': tensor([1])}, {'values': tensor([22]), 'weights': tensor([2])}, {'values': tensor([22]), 'weights': tensor([1])}, {'values': tensor([22]), 'weights': tensor([1])}, {'values': tensor([3]), 'weights': tensor([1])}]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "773a196d-e019-45b2-96f3-b7467f9c792c",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_categories_per_layer = {0: 5, 1: 10, 2: 20}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "e7d610d4-8812-429d-8c23-504fd7cea05b",
   "metadata": {},
   "outputs": [],
   "source": [
    "hierarchy_tree = {0: {0: {0: 4, 1: 11, 2: 1, 3: 3, 4: 4, 5: 3, 6: 1}, 1: {0: 3, 1: 1, 2: 1, 3: 1}}, 1: {0: {0: 7}, 1: {0: 1}}, 2: {0: {0: 1}, 1: {0: 2}}, 3: {0: {0: 1, 1: 2}, 1: {0: 1}, 2: {0: 1}}, 4: {0: {0: 1}}}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "fa938d3a-d216-4db6-a532-319703a85c88",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch import tensor\n",
    "HDP_structure = [{'0': {'values': tensor([22,  3]), 'weights': tensor([32,  1])}, '1': {'values': tensor([22]), 'weights': tensor([8])}, '2': {'values': tensor([22]), 'weights': tensor([3])}, '3': {'values': tensor([22]), 'weights': tensor([5])}, '4': {'values': tensor([3]), 'weights': tensor([1])}}, {'00': {'values': tensor([22]), 'weights': tensor([27])}, '01': {'values': tensor([22]), 'weights': tensor([6])}, '10': {'values': tensor([22]), 'weights': tensor([7])}, '11': {'values': tensor([22]), 'weights': tensor([1])}, '20': {'values': tensor([22]), 'weights': tensor([1])}, '21': {'values': tensor([22]), 'weights': tensor([2])}, '30': {'values': tensor([22]), 'weights': tensor([3])}, '31': {'values': tensor([22]), 'weights': tensor([1])}, '32': {'values': tensor([22]), 'weights': tensor([1])}, '40': {'values': tensor([3]), 'weights': tensor([1])}}, {'000': {'values': tensor([22]), 'weights': tensor([4])}, '001': {'values': tensor([22]), 'weights': tensor([11])}, '002': {'values': tensor([22]), 'weights': tensor([1])}, '003': {'values': tensor([22]), 'weights': tensor([3])}, '004': {'values': tensor([22]), 'weights': tensor([4])}, '005': {'values': tensor([22]), 'weights': tensor([3])}, '006': {'values': tensor([22]), 'weights': tensor([1])}, '010': {'values': tensor([22]), 'weights': tensor([3])}, '011': {'values': tensor([22]), 'weights': tensor([1])}, '012': {'values': tensor([22]), 'weights': tensor([1])}, '013': {'values': tensor([22]), 'weights': tensor([1])}, '100': {'values': tensor([22]), 'weights': tensor([7])}, '110': {'values': tensor([22]), 'weights': tensor([1])}, '200': {'values': tensor([22]), 'weights': tensor([1])}, '210': {'values': tensor([22]), 'weights': tensor([2])}, '300': {'values': tensor([22]), 'weights': tensor([1])}, '301': {'values': tensor([22]), 'weights': tensor([2])}, '310': {'values': tensor([22]), 'weights': tensor([1])}, '320': {'values': tensor([22]), 'weights': tensor([1])}, '400': {'values': tensor([3]), 'weights': tensor([1])}}]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "d1b8785d-874d-455a-a36d-59ea7537132e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[4, 11, 1, 3, 4, 3, 1, 3, 1, 1, 1, 7, 1, 1, 2, 1, 2, 1, 1, 1]"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "jax.tree_util.tree_leaves(hierarchy_tree)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "31e986d3-17f8-47cb-8d7c-60baf6ab6777",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "([4, 11, 1, 3, 4, 3, 1, 3, 1, 1, 1, 7, 1, 1, 2, 1, 2, 1, 1, 1],\n",
       " PyTreeDef({0: {0: {0: *, 1: *, 2: *, 3: *, 4: *, 5: *, 6: *}, 1: {0: *, 1: *, 2: *, 3: *}}, 1: {0: {0: *}, 1: {0: *}}, 2: {0: {0: *}, 1: {0: *}}, 3: {0: {0: *, 1: *}, 1: {0: *}, 2: {0: *}}, 4: {0: {0: *}}}))"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "jax.tree_util.tree_flatten(hierarchy_tree)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "920f6b42-7d43-48f1-8786-6a38098e0734",
   "metadata": {},
   "outputs": [],
   "source": [
    "leaves, struc = jax.tree_util.tree_flatten(hierarchy_tree)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "55db1c68-a7d9-4332-abbf-74f5573e0635",
   "metadata": {},
   "outputs": [],
   "source": [
    "reconstruc = jax.tree_util.tree_unflatten(struc, leaves)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "9115495b-0d28-42f5-82aa-6fe39755012e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{0: {0: {0: 4, 1: 11, 2: 1, 3: 3, 4: 4, 5: 3, 6: 1},\n",
       "  1: {0: 3, 1: 1, 2: 1, 3: 1}},\n",
       " 1: {0: {0: 7}, 1: {0: 1}},\n",
       " 2: {0: {0: 1}, 1: {0: 2}},\n",
       " 3: {0: {0: 1, 1: 2}, 1: {0: 1}, 2: {0: 1}},\n",
       " 4: {0: {0: 1}}}"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "reconstruc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "a72e76f0-3c76-4868-80a3-299a2f447eab",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{0: {0: {0: 4, 1: 11, 2: 1, 3: 3, 4: 4, 5: 3, 6: 1},\n",
       "  1: {0: 3, 1: 1, 2: 1, 3: 1}},\n",
       " 1: {0: {0: 7}, 1: {0: 1}},\n",
       " 2: {0: {0: 1}, 1: {0: 2}},\n",
       " 3: {0: {0: 1, 1: 2}, 1: {0: 1}, 2: {0: 1}},\n",
       " 4: {0: {0: 1}}}"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hierarchy_tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "fffe28bf-cace-40cb-ad4a-2364d06f5c97",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sorted tensor (rows): tensor([[1, 1, 4],\n",
      "        [2, 5, 5],\n",
      "        [3, 6, 9]])\n",
      "Indices of sorted tensor (rows): tensor([[1, 0, 0],\n",
      "        [2, 1, 2],\n",
      "        [0, 2, 1]])\n"
     ]
    }
   ],
   "source": [
    "# Example 2D tensor\n",
    "tensor_2d = torch.tensor([[3, 1, 4],\n",
    "                          [1, 5, 9],\n",
    "                          [2, 6, 5]])\n",
    "\n",
    "# Sort each row in ascending order\n",
    "sorted_tensor_rows, sorted_indices_rows = torch.sort(tensor_2d, dim=0)\n",
    "\n",
    "print(\"Sorted tensor (rows):\", sorted_tensor_rows)\n",
    "print(\"Indices of sorted tensor (rows):\", sorted_indices_rows)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "05fdf0fd-888b-43b3-84df-ddbb158157bf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[1, 1, 4],\n",
      "        [2, 5, 5],\n",
      "        [3, 5, 8],\n",
      "        [3, 6, 9]])\n"
     ]
    }
   ],
   "source": [
    "tensor = torch.tensor([[3, 1, 4],\n",
    "                       [1, 5, 9],\n",
    "                       [2, 6, 5],\n",
    "                       [3, 5, 8]])\n",
    "\n",
    "# Iterate over columns in reverse order\n",
    "for col in range(tensor.size(1) - 1, -1, -1):\n",
    "    tensor, _ = torch.sort(tensor, dim=0, descending=False)\n",
    "\n",
    "print(tensor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "c6e17700-e9eb-491d-9426-d69b6a76891e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original tensor:\n",
      " tensor([[3, 1, 4],\n",
      "        [1, 5, 9],\n",
      "        [2, 6, 5],\n",
      "        [3, 5, 8]])\n",
      "Sorted tensor:\n",
      " tensor([[1, 5, 9],\n",
      "        [2, 6, 5],\n",
      "        [3, 1, 4],\n",
      "        [3, 5, 8]])\n"
     ]
    }
   ],
   "source": [
    "tensor = torch.tensor([[3, 1, 4],\n",
    "                       [1, 5, 9],\n",
    "                       [2, 6, 5],\n",
    "                       [3, 5, 8]])\n",
    "\n",
    "# Sort tensor by columns with higher columns having lower priority\n",
    "def sort_by_columns(tensor):\n",
    "    # Iterate over columns in reverse order\n",
    "    for col in range(tensor.size(1) - 1, -1, -1):\n",
    "        tensor = tensor[tensor[:, col].argsort()]\n",
    "    return tensor\n",
    "\n",
    "# Apply the function\n",
    "sorted_tensor = sort_by_columns(tensor)\n",
    "\n",
    "print(\"Original tensor:\\n\", tensor)\n",
    "print(\"Sorted tensor:\\n\", sorted_tensor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "c4609f24-9b52-4366-9db2-a9c868d5c665",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sorted tensor:\n",
      " tensor([[1, 5, 9],\n",
      "        [2, 6, 5],\n",
      "        [3, 1, 4],\n",
      "        [3, 1, 4]])\n",
      "Positions of original rows in sorted tensor:\n",
      " tensor([2, 0, 1, 3])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "# Example tensor\n",
    "tensor = torch.tensor([[3, 1, 4],\n",
    "                       [1, 5, 9],\n",
    "                       [2, 6, 5],\n",
    "                       [3, 1, 4]])\n",
    "\n",
    "# Function to sort tensor by columns with higher columns having lower priority and return original indices\n",
    "def sort_by_columns_with_original_indices(tensor):\n",
    "    indices = torch.arange(tensor.size(0))  # Initialize the indices\n",
    "    # Iterate over columns in reverse order\n",
    "    for col in range(tensor.size(1) - 1, -1, -1):\n",
    "        sorted_indices = tensor[:, col].argsort()\n",
    "        tensor = tensor[sorted_indices]\n",
    "        indices = indices[sorted_indices]\n",
    "    # To get the positions of the original rows in the sorted tensor\n",
    "    original_positions = torch.argsort(indices)\n",
    "    return tensor, original_positions\n",
    "\n",
    "# Apply the function\n",
    "sorted_tensor, original_positions = sort_by_columns_with_original_indices(tensor)\n",
    "\n",
    "print(\"Sorted tensor:\\n\", sorted_tensor)\n",
    "print(\"Positions of original rows in sorted tensor:\\n\", original_positions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "6a2df6f3-6e93-4569-9909-630435936a40",
   "metadata": {},
   "outputs": [],
   "source": [
    "distributions = torch.tensor(jax.tree_util.tree_leaves(hierarchy_tree))\n",
    "flat_categories = torch.cumsum(distributions, dim=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "b106ac91-fd39-41c9-b3c5-444fafb28701",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([ 4, 15, 16, 19, 23, 26, 27, 30, 31, 32, 33, 40, 41, 42, 44, 45, 47, 48,\n",
       "        49, 50])"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "flat_categories"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "a28940bc-853b-4eda-9c13-371dbf89619b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([ 4, 11,  1,  3,  4,  3,  1,  3,  1,  1,  1,  7,  1,  1,  2,  1,  2,  1,\n",
       "         1,  1])"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "distributions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "4c8d651f-5061-4750-b80a-c4a7329492f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import copy\n",
    "\n",
    "\n",
    "def find_indices_of_smallest_entries_bigger_than(sorted_tensor, elements):\n",
    "    '''\n",
    "    Find the indices of the smallest entries in the sorted_tensor that are bigger than the elements\n",
    "    '''\n",
    "    # Use torch.searchsorted for binary search\n",
    "    indices = torch.searchsorted(sorted_tensor, elements, right=True)\n",
    "    # Ensure indices that exceed the length of the sorted_tensor are set to None\n",
    "    indices[indices == len(sorted_tensor)] = -1  # Use -1 to denote no such entry\n",
    "    if (torch.any(indices == -1)):\n",
    "        raise ValueError(\"Warning: Some elements exceed the largest element in the sorted_tensor\")\n",
    "    return indices\n",
    "\n",
    "\n",
    "def sort_by_columns_with_original_indices(original_labels: torch.Tensor):\n",
    "    '''\n",
    "    Get the index of labels in the flattened hierarchical tree\n",
    "    '''\n",
    "    labels = copy.deepcopy(original_labels)\n",
    "    indices = torch.arange(labels.size(0))  # Initialize the indices\n",
    "    # Iterate over columns in reverse order\n",
    "    for col in range(labels.size(1) - 1, -1, -1):\n",
    "        sorted_indices = labels[:, col].argsort()\n",
    "        labels = labels[sorted_indices]\n",
    "        indices = indices[sorted_indices]\n",
    "    # To get the positions of the original rows in the sorted tensor\n",
    "    original_positions = torch.argsort(indices)\n",
    "    return original_positions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "9e64a5ff-8ef6-491a-8891-2b53f29887f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "distributions = torch.tensor(jax.tree_util.tree_leaves(hierarchy_tree))\n",
    "flat_categories = torch.cumsum(distributions, dim=0)\n",
    "sorted_indices = sort_by_columns_with_original_indices(labels)\n",
    "category_indices = find_indices_of_smallest_entries_bigger_than(flat_categories, sorted_indices)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "83497045-ecef-42a1-8a72-00dd426a9e07",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([ 4, 11,  1,  3,  4,  3,  1,  3,  1,  1,  1,  7,  1,  1,  2,  1,  2,  1,\n",
       "         1,  1])"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "distributions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "6f2561bb-1f2c-4f66-82be-e7ade6a61f0f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([ 4, 15, 16, 19, 23, 26, 27, 30, 31, 32, 33, 40, 41, 42, 44, 45, 47, 48,\n",
       "        49, 50])"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "flat_categories"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "4c1590f3-83a2-44a2-a679-592e7ee1848c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([ 0,  1, 34,  3, 30, 22, 43, 40, 20, 15, 13, 46, 31,  4, 10, 37, 25,  5,\n",
       "        24, 44, 36, 23, 47, 35, 18, 14, 26, 27, 19, 17, 33, 16, 39,  2, 38, 12,\n",
       "        28, 49, 21, 48, 29, 45,  6,  9, 42,  7,  8, 32, 41, 11])"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sorted_indices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "00b01f1b-e7d1-4b74-89d3-708f20c8391a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([ 0,  0, 11,  0,  8,  4, 14, 12,  4,  2,  1, 16,  9,  1,  1, 11,  5,  1,\n",
       "         5, 15, 11,  5, 17, 11,  3,  1,  6,  7,  4,  3, 11,  3, 11,  0, 11,  1,\n",
       "         7, 19,  4, 18,  7, 16,  1,  1, 14,  1,  1, 10, 13,  1])"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "category_indices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "978fac96-b979-4cb4-a167-6b129bc563fc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "20"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(flat_categories)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed2b79e3-aa8b-4b50-affb-52dbf4b2e92d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:hie_reg]",
   "language": "python",
   "name": "conda-env-hie_reg-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
